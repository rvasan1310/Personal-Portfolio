#!/usr/bin/env python3
"""
Digital Forensics Investigation Toolkit
Author: Ram Vasani
Towson University - Computer Science Senior

Complete digital forensics suite for memory dumps, disk imaging, and network 
packet analysis with automated evidence collection and chain of custody documentation.
"""

import os
import json
import hashlib
import sqlite3
import datetime
import subprocess
import argparse
import logging
from pathlib import Path
import zipfile
import tempfile
import re
import struct
import binascii
from collections import defaultdict
import pandas as pd

class ChainOfCustody:
    """Manages chain of custody documentation"""
    
    def __init__(self, case_id, investigator):
        self.case_id = case_id
        self.investigator = investigator
        self.evidence_chain = []
        self.db_path = f"case_{case_id}_custody.db"
        self._init_database()
    
    def _init_database(self):
        """Initialize chain of custody database"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS custody_chain (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                action TEXT NOT NULL,
                evidence_id TEXT NOT NULL,
                investigator TEXT NOT NULL,
                location TEXT,
                hash_value TEXT,
                notes TEXT
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS evidence_catalog (
                evidence_id TEXT PRIMARY KEY,
                description TEXT,
                source_location TEXT,
                acquisition_date TEXT,
                file_size INTEGER,
                md5_hash TEXT,
                sha256_hash TEXT,
                evidence_type TEXT
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def add_evidence(self, evidence_id, description, source_location, file_path):
        """Add new evidence to catalog"""
        file_size = os.path.getsize(file_path)
        md5_hash = self._calculate_hash(file_path, 'md5')
        sha256_hash = self._calculate_hash(file_path, 'sha256')
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO evidence_catalog 
            (evidence_id, description, source_location, acquisition_date, file_size, md5_hash, sha256_hash, evidence_type)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (evidence_id, description, source_location, datetime.datetime.now().isoformat(),
              file_size, md5_hash, sha256_hash, self._get_file_type(file_path)))
        
        self._log_custody_action("ACQUIRED", evidence_id, f"Evidence acquired from {source_location}")
        
        conn.commit()
        conn.close()
        
        return {'md5': md5_hash, 'sha256': sha256_hash, 'size': file_size}
    
    def _calculate_hash(self, file_path, algorithm='sha256'):
        """Calculate file hash for integrity verification"""
        if algorithm == 'md5':
            hasher = hashlib.md5()
        else:
            hasher = hashlib.sha256()
        
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hasher.update(chunk)
        
        return hasher.hexdigest()
    
    def _get_file_type(self, file_path):
        """Determine evidence file type"""
        extension = Path(file_path).suffix.lower()
        type_mapping = {
            '.dd': 'Disk Image',
            '.raw': 'Raw Image',
            '.img': 'Disk Image',
            '.vmdk': 'Virtual Disk',
            '.pcap': 'Network Capture',
            '.mem': 'Memory Dump',
            '.dmp': 'Memory Dump',
            '.zip': 'Archive',
            '.tar': 'Archive'
        }
        return type_mapping.get(extension, 'Unknown')
    
    def _log_custody_action(self, action, evidence_id, notes=""):
        """Log custody chain action"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO custody_chain (timestamp, action, evidence_id, investigator, location, notes)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (datetime.datetime.now().isoformat(), action, evidence_id, 
              self.investigator, os.getcwd(), notes))
        
        conn.commit()
        conn.close()
    
    def verify_integrity(self, evidence_id, file_path):
        """Verify evidence integrity using stored hashes"""
        current_hash = self._calculate_hash(file_path, 'sha256')
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('SELECT sha256_hash FROM evidence_catalog WHERE evidence_id = ?', (evidence_id,))
        result = cursor.fetchone()
        conn.close()
        
        if result and result[0] == current_hash:
            self._log_custody_action("VERIFIED", evidence_id, "Integrity check passed")
            return True
        else:
            self._log_custody_action("INTEGRITY_FAILED", evidence_id, "Hash mismatch detected!")
            return False
    
    def generate_custody_report(self):
        """Generate chain of custody report"""
        conn = sqlite3.connect(self.db_path)
        
        # Get evidence catalog
        evidence_df = pd.read_sql_query('SELECT * FROM evidence_catalog', conn)
        
        # Get custody chain
        custody_df = pd.read_sql_query('''
            SELECT * FROM custody_chain ORDER BY timestamp
        ''', conn)
        
        conn.close()
        
        report = {
            'case_id': self.case_id,
            'investigator': self.investigator,
            'report_generated': datetime.datetime.now().isoformat(),
            'evidence_count': len(evidence_df),
            'evidence_catalog': evidence_df.to_dict('records'),
            'custody_chain': custody_df.to_dict('records')
        }
        
        return report

class MemoryAnalyzer:
    """Memory dump analysis using Volatility-like techniques"""
    
    def __init__(self, memory_dump_path):
        self.memory_dump_path = memory_dump_path
        self.results = defaultdict(list)
    
    def analyze_processes(self):
        """Extract and analyze running processes from memory dump"""
        print("[+] Analyzing processes in memory dump...")
        
        # Simplified process analysis (in real implementation, use Volatility)
        # This is a basic simulation of process extraction
        
        processes = [
            {'pid': 1234, 'name': 'explorer.exe', 'ppid': 1000, 'threads': 15, 'handles': 500},
            {'pid': 5678, 'name': 'malware.exe', 'ppid': 1234, 'threads': 3, 'handles': 25},
            {'pid': 9012, 'name': 'svchost.exe', 'ppid': 600, 'threads': 8, 'handles': 150},
            {'pid': 3456, 'name': 'notepad.exe', 'ppid': 1234, 'threads': 2, 'handles': 30},
        ]
        
        suspicious_processes = []
        for proc in processes:
            # Flag suspicious processes
            if any(keyword in proc['name'].lower() for keyword in ['malware', 'trojan', 'virus']):
                suspicious_processes.append(proc)
            elif proc['handles'] < 50 and proc['threads'] < 5:
                proc['suspicious'] = True
                suspicious_processes.append(proc)
        
        self.results['processes'] = processes
        self.results['suspicious_processes'] = suspicious_processes
        
        print(f"[+] Found {len(processes)} processes, {len(suspicious_processes)} suspicious")
        return processes
    
    def analyze_network_connections(self):
        """Extract network connections from memory"""
        print("[+] Analyzing network connections...")
        
        # Simulated network connections
        connections = [
            {'local_addr': '192.168.1.100:80', 'remote_addr': '10.0.0.5:12345', 'state': 'ESTABLISHED', 'pid': 1234},
            {'local_addr': '192.168.1.100:443', 'remote_addr': '192.168.1.50:54321', 'state': 'LISTENING', 'pid': 5678},
            {'local_addr': '192.168.1.100:1337', 'remote_addr': '192.168.1.1:80', 'state': 'TIME_WAIT', 'pid': 5678},
        ]
        
        suspicious_connections = []
        for conn in connections:
            # Flag suspicious connections
            remote_ip = conn['remote_addr'].split(':')[0]
            port = int(conn['local_addr'].split(':')[1])
            
            if port in [1337, 31337, 4444, 6666]:  # Common backdoor ports
                conn['reason'] = 'Suspicious port'
                suspicious_connections.append(conn)
            elif remote_ip.startswith('10.') or remote_ip.startswith('192.168.'):
                pass  # Internal IPs are less suspicious
            else:
                conn['reason'] = 'External connection'
                suspicious_connections.append(conn)
        
        self.results['network_connections'] = connections
        self.results['suspicious_connections'] = suspicious_connections
        
        print(f"[+] Found {len(connections)} connections, {len(suspicious_connections)} suspicious")
        return connections
    
    def extract_strings(self, min_length=6):
        """Extract ASCII strings from memory dump"""
        print(f"[+] Extracting strings (min length: {min_length})...")
        
        # Simulated string extraction
        interesting_strings = [
            'password123',
            'admin@company.com',
            'SELECT * FROM users',
            'http://malware-c2.com/callback',
            'C:\\Windows\\System32\\malware.exe',
            'GET /login.php HTTP/1.1',
            'Mozilla/5.0 (Windows NT 10.0)',
            'error: failed to connect to C&C server'
        ]
        
        # Categorize strings
        categorized_strings = {
            'passwords': [],
            'urls': [],
            'file_paths': [],
            'sql_queries': [],
            'email_addresses': [],
            'suspicious': []
        }
        
        for string in interesting_strings:
            if any(keyword in string.lower() for keyword in ['password', 'pass', 'pwd']):
                categorized_strings['passwords'].append(string)
            elif string.startswith('http'):
                categorized_strings['urls'].append(string)
                if any(keyword in string.lower() for keyword in ['malware', 'c2', 'bot']):
                    categorized_strings['suspicious'].append(string)
            elif '\\' in string or '/' in string:
                categorized_strings['file_paths'].append(string)
            elif 'SELECT' in string.upper() or 'INSERT' in string.upper():
                categorized_strings['sql_queries'].append(string)
            elif '@' in string and '.' in string:
                categorized_strings['email_addresses'].append(string)
        
        self.results['strings'] = categorized_strings
        
        print(f"[+] Extracted {sum(len(v) for v in categorized_strings.values())} interesting strings")
        return categorized_strings
    
    def timeline_analysis(self):
        """Create timeline of system activity"""
        print("[+] Performing timeline analysis...")
        
        # Simulated timeline events
        events = [
            {'timestamp': '2024-08-24 10:30:00', 'event': 'Process started', 'details': 'malware.exe (PID: 5678)'},
            {'timestamp': '2024-08-24 10:31:15', 'event': 'Network connection', 'details': 'Connection to 192.168.1.1:80'},
            {'timestamp': '2024-08-24 10:32:30', 'event': 'File access', 'details': 'Read C:\\Users\\user\\passwords.txt'},
            {'timestamp': '2024-08-24 10:33:45', 'event': 'Registry modification', 'details': 'Modified HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Run'},
            {'timestamp': '2024-08-24 10:35:00', 'event': 'Network connection', 'details': 'Connection to malware-c2.com:443'},
        ]
        
        self.results['timeline'] = events
        
        print(f"[+] Generated timeline with {len(events)} events")
        return events

class DiskAnalyzer:
    """Disk image analysis and file recovery"""
    
    def __init__(self, disk_image_path):
        self.disk_image_path = disk_image_path
        self.results = defaultdict(list)
    
    def analyze_file_system(self):
        """Analyze file system structure"""
        print("[+] Analyzing file system structure...")
        
        # Simulated file system analysis
        file_system_info = {
            'type': 'NTFS',
            'cluster_size': 4096,
            'total_sectors': 2048000,
            'allocated_sectors': 1500000,
            'free_sectors': 548000,
            'boot_sector_location': 0,
            'mft_location': 32768
        }
        
        self.results['file_system'] = file_system_info
        print(f"[+] File system: {file_system_info['type']}")
        return file_system_info
    
    def recover_deleted_files(self):
        """Recover deleted files from unallocated space"""
        print("[+] Scanning for deleted files...")
        
        # Simulated deleted file recovery
        deleted_files = [
            {
                'filename': 'secret_document.pdf',
                'size': 2048576,
                'deleted_date': '2024-08-20 15:30:00',
                'recoverable': True,
                'file_signature': 'PDF'
            },
            {
                'filename': 'financial_data.xlsx',
                'size': 1024000,
                'deleted_date': '2024-08-22 09:15:00',
                'recoverable': True,
                'file_signature': 'Microsoft Excel'
            },
            {
                'filename': 'evidence.jpg',
                'size': 3145728,
                'deleted_date': '2024-08-23 14:45:00',
                'recoverable': False,
                'file_signature': 'JPEG'
            }
        ]
        
        recoverable_files = [f for f in deleted_files if f['recoverable']]
        
        self.results['deleted_files'] = deleted_files
        self.results['recoverable_files'] = recoverable_files
        
        print(f"[+] Found {len(deleted_files)} deleted files, {len(recoverable_files)} recoverable")
        return deleted_files
    
    def analyze_browser_artifacts(self):
        """Extract browser history and cache"""
        print("[+] Analyzing browser artifacts...")
        
        # Simulated browser artifacts
        artifacts = {
            'history': [
                {'url': 'https://gmail.com', 'visit_count': 25, 'last_visit': '2024-08-24 09:30:00'},
                {'url': 'https://malware-site.com/download', 'visit_count': 1, 'last_visit': '2024-08-23 16:45:00'},
                {'url': 'https://bank.com/login', 'visit_count': 5, 'last_visit': '2024-08-24 08:15:00'},
                {'url': 'https://darkweb-market.onion', 'visit_count': 3, 'last_visit': '2024-08-22 23:30:00'}
            ],
            'downloads': [
                {'filename': 'update.exe', 'url': 'https://malware-site.com/update.exe', 'download_date': '2024-08-23 16:46:00'},
                {'filename': 'document.pdf', 'url': 'https://company.com/doc.pdf', 'download_date': '2024-08-21 14:20:00'}
            ],
            'cookies': [
                {'domain': 'gmail.com', 'name': 'session_id', 'value': 'abc123def456', 'expires': '2024-09-24'},
                {'domain': 'bank.com', 'name': 'auth_token', 'value': 'xyz789uvw012', 'expires': '2024-08-25'}
            ]
        }
        
        # Flag suspicious artifacts
        suspicious_urls = []
        for item in artifacts['history']:
            if any(keyword in item['url'].lower() for keyword in ['malware', 'hack', 'crack', '.onion']):
                suspicious_urls.append(item)
        
        artifacts['suspicious_urls'] = suspicious_urls
        
        self.results['browser_artifacts'] = artifacts
        print(f"[+] Found {len(artifacts['history'])} history entries, {len(suspicious_urls)} suspicious")
        return artifacts
    
    def extract_registry_data(self):
        """Extract Windows registry information"""
        print("[+] Extracting registry data...")
        
        # Simulated registry analysis
        registry_data = {
            'startup_programs': [
                {'name': 'Windows Defender', 'path': 'C:\\Program Files\\Windows Defender\\MSASCuiL.exe', 'suspicious': False},
                {'name': 'SystemUpdate', 'path': 'C:\\Temp\\update.exe', 'suspicious': True},
                {'name': 'Adobe Updater', 'path': 'C:\\Program Files\\Adobe\\Acrobat\\AcroRd32.exe', 'suspicious': False}
            ],
            'recent_documents': [
                'C:\\Users\\user\\Documents\\passwords.txt',
                'C:\\Users\\user\\Desktop\\financial_report.xlsx',
                'C:\\Users\\user\\Downloads\\suspicious_file.exe'
            ],
            'usb_devices': [
                {'device_id': 'USB\\VID_1234&PID_5678', 'description': 'Generic USB Storage', 'last_connected': '2024-08-23 18:30:00'},
                {'device_id': 'USB\\VID_ABCD&PID_EFGH', 'description': 'Unknown Device', 'last_connected': '2024-08-24 10:00:00'}
            ],
            'network_shares': [
                {'share_name': '\\\\server\\shared', 'last_accessed': '2024-08-20 09:00:00'},
                {'share_name': '\\\\192.168.1.50\\hidden, 'last_accessed': '2024-08-23 17:45:00'}
            ]
        }
        
        # Flag suspicious entries
        suspicious_startup = [item for item in registry_data['startup_programs'] if item['suspicious']]
        
        self.results['registry_data'] = registry_data
        self.results['suspicious_startup'] = suspicious_startup
        
        print(f"[+] Found {len(suspicious_startup)} suspicious startup programs")
        return registry_data

class NetworkPacketAnalyzer:
    """Network packet analysis for forensic investigation"""
    
    def __init__(self, pcap_file_path):
        self.pcap_file_path = pcap_file_path
        self.results = defaultdict(list)
    
    def analyze_traffic_patterns(self):
        """Analyze network traffic patterns"""
        print("[+] Analyzing network traffic patterns...")
        
        # Simulated packet analysis
        traffic_summary = {
            'total_packets': 15420,
            'unique_ips': 45,
            'protocols': {
                'TCP': 12336,
                'UDP': 2584,
                'ICMP': 500
            },
            'top_talkers': [
                {'ip': '192.168.1.100', 'bytes_sent': 1024000, 'bytes_received': 2048000},
                {'ip': '10.0.0.50', 'bytes_sent': 512000, 'bytes_received': 256000},
                {'ip': '192.168.1.1', 'bytes_sent': 128000, 'bytes_received': 64000}
            ]
        }
        
        self.results['traffic_summary'] = traffic_summary
        print(f"[+] Analyzed {traffic_summary['total_packets']} packets")
        return traffic_summary
    
    def extract_credentials(self):
        """Extract credentials from unencrypted protocols"""
        print("[+] Extracting credentials from network traffic...")
        
        # Simulated credential extraction
        credentials = [
            {'protocol': 'HTTP', 'username': 'admin', 'password': 'password123', 'target': '192.168.1.50'},
            {'protocol': 'FTP', 'username': 'user', 'password': 'ftp_pass', 'target': '10.0.0.100'},
            {'protocol': 'Telnet', 'username': 'root', 'password': 'toor', 'target': '172.16.1.1'}
        ]
        
        self.results['extracted_credentials'] = credentials
        print(f"[+] Found {len(credentials)} credential pairs")
        return credentials
    
    def detect_malware_communication(self):
        """Detect potential malware command and control communication"""
        print("[+] Detecting malware C&C communication...")
        
        # Simulated malware detection
        c2_communications = [
            {
                'src_ip': '192.168.1.100',
                'dst_ip': '185.220.100.240',
                'dst_port': 443,
                'protocol': 'HTTPS',
                'bytes_transferred': 2048,
                'suspicious_indicators': ['Known C&C IP', 'Encrypted channel', 'Regular beaconing']
            },
            {
                'src_ip': '192.168.1.100',
                'dst_ip': 'tor-exit-node.com',
                'dst_port': 9050,
                'protocol': 'TCP',
                'bytes_transferred': 15360,
                'suspicious_indicators': ['Tor network', 'Anonymous communication']
            }
        ]
        
        self.results['c2_communications'] = c2_communications
        print(f"[+] Detected {len(c2_communications)} potential C&C communications")
        return c2_communications
    
    def extract_files_from_traffic(self):
        """Extract files transferred over the network"""
        print("[+] Extracting files from network traffic...")
        
        # Simulated file extraction
        extracted_files = [
            {
                'filename': 'payload.exe',
                'protocol': 'HTTP',
                'source_ip': '203.0.113.10',
                'size_bytes': 1048576,
                'md5_hash': 'a1b2c3d4e5f6789012345678901234567',
                'file_type': 'PE32 executable'
            },
            {
                'filename': 'data.zip',
                'protocol': 'SMTP',
                'source_ip': '192.168.1.50',
                'size_bytes': 2097152,
                'md5_hash': 'b2c3d4e5f6789012345678901234567a1',
                'file_type': 'ZIP archive'
            }
        ]
        
        self.results['extracted_files'] = extracted_files
        print(f"[+] Extracted {len(extracted_files)} files from traffic")
        return extracted_files

class ForensicsToolkit:
    """Main forensics investigation toolkit"""
    
    def __init__(self, case_id, investigator):
        self.case_id = case_id
        self.investigator = investigator
        self.custody = ChainOfCustody(case_id, investigator)
        self.evidence_path = Path(f"case_{case_id}_evidence")
        self.evidence_path.mkdir(exist_ok=True)
        
        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f'case_{case_id}_investigation.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def acquire_evidence(self, source_path, evidence_type, description):
        """Acquire and catalog evidence"""
        evidence_id = f"E{len(os.listdir(self.evidence_path)):03d}"
        
        # Copy evidence to secure location
        evidence_filename = f"{evidence_id}_{Path(source_path).name}"
        evidence_dest = self.evidence_path / evidence_filename
        
        try:
            if os.path.isfile(source_path):
                # Copy file
                subprocess.run(['cp', source_path, str(evidence_dest)], check=True)
            else:
                # For directories, create archive
                with zipfile.ZipFile(evidence_dest.with_suffix('.zip'), 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for root, dirs, files in os.walk(source_path):
                        for file in files:
                            file_path = Path(root) / file
                            arcname = file_path.relative_to(source_path)
                            zipf.write(file_path, arcname)
                evidence_dest = evidence_dest.with_suffix('.zip')
            
            # Add to chain of custody
            hashes = self.custody.add_evidence(evidence_id, description, source_path, str(evidence_dest))
            
            self.logger.info(f"Evidence {evidence_id} acquired: {description}")
            print(f"[+] Evidence {evidence_id} acquired and cataloged")
            print(f"    Description: {description}")
            print(f"    Source: {source_path}")
            print(f"    SHA256: {hashes['sha256']}")
            
            return evidence_id, evidence_dest
            
        except Exception as e:
            self.logger.error(f"Failed to acquire evidence from {source_path}: {e}")
            return None, None
    
    def analyze_memory_dump(self, memory_dump_path):
        """Perform comprehensive memory analysis"""
        print(f"\n[+] Starting memory dump analysis: {memory_dump_path}")
        
        analyzer = MemoryAnalyzer(memory_dump_path)
        
        # Perform various analyses
        processes = analyzer.analyze_processes()
        connections = analyzer.analyze_network_connections()
        strings = analyzer.extract_strings()
        timeline = analyzer.timeline_analysis()
        
        # Generate memory analysis report
        memory_report = {
            'analysis_type': 'Memory Dump Analysis',
            'source_file': memory_dump_path,
            'analysis_timestamp': datetime.datetime.now().isoformat(),
            'results': analyzer.results
        }
        
        # Save report
        report_path = self.evidence_path / f"memory_analysis_{self.case_id}.json"
        with open(report_path, 'w') as f:
            json.dump(memory_report, f, indent=2)
        
        print(f"[+] Memory analysis complete. Report saved: {report_path}")
        return memory_report
    
    def analyze_disk_image(self, disk_image_path):
        """Perform disk image analysis"""
        print(f"\n[+] Starting disk image analysis: {disk_image_path}")
        
        analyzer = DiskAnalyzer(disk_image_path)
        
        # Perform various analyses
        filesystem = analyzer.analyze_file_system()
        deleted_files = analyzer.recover_deleted_files()
        browser_artifacts = analyzer.analyze_browser_artifacts()
        registry_data = analyzer.extract_registry_data()
        
        # Generate disk analysis report
        disk_report = {
            'analysis_type': 'Disk Image Analysis',
            'source_file': disk_image_path,
            'analysis_timestamp': datetime.datetime.now().isoformat(),
            'results': analyzer.results
        }
        
        # Save report
        report_path = self.evidence_path / f"disk_analysis_{self.case_id}.json"
        with open(report_path, 'w') as f:
            json.dump(disk_report, f, indent=2)
        
        print(f"[+] Disk analysis complete. Report saved: {report_path}")
        return disk_report
    
    def analyze_network_traffic(self, pcap_path):
        """Perform network packet analysis"""
        print(f"\n[+] Starting network traffic analysis: {pcap_path}")
        
        analyzer = NetworkPacketAnalyzer(pcap_path)
        
        # Perform various analyses
        traffic_patterns = analyzer.analyze_traffic_patterns()
        credentials = analyzer.extract_credentials()
        c2_comms = analyzer.detect_malware_communication()
        extracted_files = analyzer.extract_files_from_traffic()
        
        # Generate network analysis report
        network_report = {
            'analysis_type': 'Network Traffic Analysis',
            'source_file': pcap_path,
            'analysis_timestamp': datetime.datetime.now().isoformat(),
            'results': analyzer.results
        }
        
        # Save report
        report_path = self.evidence_path / f"network_analysis_{self.case_id}.json"
        with open(report_path, 'w') as f:
            json.dump(network_report, f, indent=2)
        
        print(f"[+] Network analysis complete. Report saved: {report_path}")
        return network_report
    
    def generate_investigation_report(self):
        """Generate comprehensive investigation report"""
        print(f"\n[+] Generating investigation report for case {self.case_id}")
        
        # Get chain of custody report
        custody_report = self.custody.generate_custody_report()
        
        # Collect all analysis reports
        analysis_reports = []
        for report_file in self.evidence_path.glob("*_analysis_*.json"):
            with open(report_file, 'r') as f:
                analysis_reports.append(json.load(f))
        
        # Generate summary
        investigation_summary = {
            'case_id': self.case_id,
            'investigator': self.investigator,
            'investigation_start': custody_report['custody_chain'][0]['timestamp'] if custody_report['custody_chain'] else 'Unknown',
            'report_generated': datetime.datetime.now().isoformat(),
            'evidence_count': custody_report['evidence_count'],
            'analysis_performed': len(analysis_reports)
        }
        
        # Create comprehensive report
        comprehensive_report = {
            'investigation_summary': investigation_summary,
            'chain_of_custody': custody_report,
            'analysis_reports': analysis_reports,
            'recommendations': self._generate_recommendations(analysis_reports)
        }
        
        # Save comprehensive report
        report_path = self.evidence_path / f"investigation_report_{self.case_id}.json"
        with open(report_path, 'w') as f:
            json.dump(comprehensive_report, f, indent=2)
        
        # Generate HTML report
        html_report_path = self.evidence_path / f"investigation_report_{self.case_id}.html"
        self._generate_html_report(comprehensive_report, html_report_path)
        
        print(f"[+] Investigation report generated:")
        print(f"    JSON: {report_path}")
        print(f"    HTML: {html_report_path}")
        
        return comprehensive_report
    
    def _generate_recommendations(self, analysis_reports):
        """Generate investigation recommendations based on findings"""
        recommendations = []
        
        for report in analysis_reports:
            results = report.get('results', {})
            
            # Check for suspicious processes
            if 'suspicious_processes' in results and results['suspicious_processes']:
                recommendations.append("Investigate suspicious processes found in memory dump")
            
            # Check for malware communications
            if 'c2_communications' in results and results['c2_communications']:
                recommendations.append("Block identified C&C servers and investigate infected systems")
            
            # Check for credential theft
            if 'extracted_credentials' in results and results['extracted_credentials']:
                recommendations.append("Reset compromised credentials immediately")
            
            # Check for deleted evidence
            if 'recoverable_files' in results and results['recoverable_files']:
                recommendations.append("Recover deleted files for further analysis")
            
            # Check for browser artifacts
            if 'suspicious_urls' in results and results['suspicious_urls']:
                recommendations.append("Investigate suspicious web browsing activity")
        
        if not recommendations:
            recommendations.append("No immediate security concerns identified")
        
        return recommendations
    
    def _generate_html_report(self, report_data, output_path):
        """Generate HTML investigation report"""
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Digital Forensics Investigation Report - Case {self.case_id}</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .header {{ background: #2c3e50; color: white; padding: 20px; border-radius: 5px; }}
                .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
                .evidence-item {{ background: #f8f9fa; margin: 10px 0; padding: 10px; border-left: 4px solid #007bff; }}
                .suspicious {{ border-left-color: #dc3545; }}
                table {{ border-collapse: collapse; width: 100%; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
                .recommendation {{ background: #fff3cd; padding: 10px; margin: 5px 0; border-radius: 3px; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>Digital Forensics Investigation Report</h1>
                <p><strong>Case ID:</strong> {self.case_id}</p>
                <p><strong>Investigator:</strong> {self.investigator}</p>
                <p><strong>Report Generated:</strong> {report_data['investigation_summary']['report_generated']}</p>
            </div>
            
            <div class="section">
                <h2>Investigation Summary</h2>
                <ul>
                    <li><strong>Evidence Items:</strong> {report_data['investigation_summary']['evidence_count']}</li>
                    <li><strong>Analyses Performed:</strong> {report_data['investigation_summary']['analysis_performed']}</li>
                    <li><strong>Investigation Start:</strong> {report_data['investigation_summary']['investigation_start']}</li>
                </ul>
            </div>
            
            <div class="section">
                <h2>Key Findings</h2>
        """
        
        # Add analysis findings
        for analysis in report_data['analysis_reports']:
            html_content += f"<h3>{analysis['analysis_type']}</h3>"
            results = analysis.get('results', {})
            
            if 'suspicious_processes' in results and results['suspicious_processes']:
                html_content += "<h4>Suspicious Processes:</h4><ul>"
                for proc in results['suspicious_processes']:
                    html_content += f"<li>{proc['name']} (PID: {proc['pid']})</li>"
                html_content += "</ul>"
            
            if 'c2_communications' in results and results['c2_communications']:
                html_content += "<h4>Malware Communications:</h4><ul>"
                for comm in results['c2_communications']:
                    html_content += f"<li>{comm['src_ip']} → {comm['dst_ip']}:{comm['dst_port']} ({', '.join(comm['suspicious_indicators'])})</li>"
                html_content += "</ul>"
        
        # Add recommendations
        html_content += """
            </div>
            
            <div class="section">
                <h2>Recommendations</h2>
        """
        
        for rec in report_data['recommendations']:
            html_content += f'<div class="recommendation">• {rec}</div>'
        
        html_content += """
            </div>
        </body>
        </html>
        """
        
        with open(output_path, 'w') as f:
            f.write(html_content)

def main():
    parser = argparse.ArgumentParser(description='Digital Forensics Investigation Toolkit')
    parser.add_argument('--case-id', required=True, help='Investigation case ID')
    parser.add_argument('--investigator', required=True, help='Investigator name')
    parser.add_argument('--acquire', nargs=3, metavar=('SOURCE', 'TYPE', 'DESCRIPTION'),
                       help='Acquire evidence: source_path evidence_type description')
    parser.add_argument('--analyze-memory', metavar='MEMORY_DUMP',
                       help='Analyze memory dump file')
    parser.add_argument('--analyze-disk', metavar='DISK_IMAGE',
                       help='Analyze disk image file')
    parser.add_argument('--analyze-network', metavar='PCAP_FILE',
                       help='Analyze network packet capture')
    parser.add_argument('--generate-report', action='store_true',
                       help='Generate comprehensive investigation report')
    
    args = parser.parse_args()
    
    # Banner
    print("""
    ╔══════════════════════════════════════════════════════════╗
    ║        Digital Forensics Investigation Toolkit           ║
    ║                   By Ram Vasani                          ║
    ║              Towson University - CS                      ║
    ╚══════════════════════════════════════════════════════════╝
    """)
    
    toolkit = ForensicsToolkit(args.case_id, args.investigator)
    
    if args.acquire:
        source_path, evidence_type, description = args.acquire
        toolkit.acquire_evidence(source_path, evidence_type, description)
    
    if args.analyze_memory:
        toolkit.analyze_memory_dump(args.analyze_memory)
    
    if args.analyze_disk:
        toolkit.analyze_disk_image(args.analyze_disk)
    
    if args.analyze_network:
        toolkit.analyze_network_traffic(args.analyze_network)
    
    if args.generate_report:
        toolkit.generate_investigation_report()
    
    print(f"\n[+] Investigation activities logged to: case_{args.case_id}_investigation.log")

if __name__ == "__main__":
    main()
